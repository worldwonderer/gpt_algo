<!DOCTYPE html>
<html lang="zh">
<head>
    <title>多线程网页爬虫</title>
    <link rel="shortcut icon" href="/static/favicon.ico">
    <link href="/static/css/bootstrap.min.css" rel="stylesheet">
    <link rel="stylesheet" href="/static/css/detail.css">
    <link rel="stylesheet" href="/static/css/github.css">
    <style>
        body {
            height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container-fluid {
            flex-grow: 1;
            display: flex;
            flex-direction: column;
        }
        main {
            flex-grow: 1;
        }
        .code-pre {
             white-space: pre-wrap;
             word-wrap: break-word;
        }
    </style>
</head>
<body>
    <div class="container-fluid mt-3">
        <header class="d-flex flex-column flex-md-row justify-content-md-between align-items-md-center mb-3">
            <a href="/problems" class="btn btn-secondary mb-2 mb-md-0">返回题目列表</a>
            <div class="text-md-end">
                <strong class="me-md-3 d-block d-md-inline mb-2 mb-md-0">多线程网页爬虫</strong>
                标签:
                
                    <a href="/problems?tag=depth-first-search" class="badge bg-secondary tag-link">深度优先搜索</a>
                
                    <a href="/problems?tag=breadth-first-search" class="badge bg-secondary tag-link">广度优先搜索</a>
                
                    <a href="/problems?tag=concurrency" class="badge bg-secondary tag-link">多线程</a>
                
                &nbsp;&nbsp;
                难度: <span class="badge bg-secondary">Medium</span>
            </div>
        </header>

        <main>
            <section class="mb-4">
                <h2>Submission</h2>
                <div class="code-block">
                    <pre class="bg-light p-2 code-pre"><code class="language-python"># &#34;&#34;&#34;
# This is HtmlParser&#39;s API interface.
# You should not implement it, or speculate about its implementation
# &#34;&#34;&#34;
#class HtmlParser(object):
#    def getUrls(self, url):
#        &#34;&#34;&#34;
#        :type url: str
#        :rtype List[str]
#        &#34;&#34;&#34;
from queue import Queue
from concurrent.futures import ThreadPoolExecutor
import threading
import concurrent

def get_hostname(url):
    # url = url[len(&#39;http://&#39;):]
    # hostname = f&#39;http://{url.split(&#34;/&#34;)[0]}&#39;
    # return hostname
    return url.split(&#39;/&#39;)[2]


class Solution:
    def crawl(self, startUrl: str, htmlParser: &#39;HtmlParser&#39;) -&gt; List[str]:
        hostname = get_hostname(startUrl)

        seen = set()
        seen.add(startUrl)

        to_visit = [startUrl]
        res = [startUrl]

        while(to_visit):
            futures = []
            # Explore candidates of ALL urls in the queue, with parallelism.
            with ThreadPoolExecutor(max_workers=16) as executor:
                for url in to_visit:
                    futures.append(executor.submit(htmlParser.getUrls, url))
            
            to_visit = []
            for future in concurrent.futures.as_completed(futures):
                for c in future.result():
                    if c not in seen and get_hostname(c) == hostname:
                        seen.add(c)
                        res.append(c)
                        to_visit.append(c)
        return res

</code></pre>
                    <button class="btn btn-sm btn-secondary copy-btn" onclick="copyCode(this)">复制代码</button>
                    <p class="mt-2 mb-0">运行时间: 206 ms</p>
                    <p class="mb-0">内存: 0.0 MB</p>
                </div>
            </section>

            
                <section>
                    <h2>Explain</h2>
                    <div>
                        <p>该题解采用了多线程来加速网页爬虫的过程。首先，定义了一个帮助函数 `get_hostname` 用来从 URL 中提取主机名。在主函数 `crawl` 中，首先使用 `get_hostname` 得到初始 URL 的主机名。然后，使用一个集合 `seen` 来跟踪已经访问过的 URL，避免重复访问。接下来，使用列表 `to_visit` 来存储当前层待访问的 URL，并使用列表 `res` 来存储所有被访问过的 URL。对于 `to_visit` 中的每个 URL，使用线程池 `ThreadPoolExecutor` 来并行获取每个 URL 链接到的新 URL。对于每个新 URL，如果它未被访问过且与起始 URL 属于同一主机，就将其添加到 `seen`，`res`，以及下一轮的 `to_visit` 中。这个过程重复进行，直到没有新的 URL 可以访问。</p>
                        <p>时间复杂度: O(n^2)</p>
                        <p>空间复杂度: O(n)</p>
                        
                             <pre class="bg-light p-2 code-pre"><code class="language-python"># 定义获取主机名的函数
from queue import Queue
from concurrent.futures import ThreadPoolExecutor
import threading
import concurrent

def get_hostname(url):
    # 从 URL 中提取主机名部分
    return url.split(&#39;/&#39;)[2]


class Solution:
    def crawl(self, startUrl: str, htmlParser: &#39;HtmlParser&#39;) -&gt; List[str]:
        # 获取起始 URL 的主机名
        hostname = get_hostname(startUrl)

        # 初始化已访问集合和结果列表
        seen = set()
        seen.add(startUrl)

        # 初始化待访问列表
        to_visit = [startUrl]
        res = [startUrl]

        # 当还有未访问的 URL 时继续
        while(to_visit):
            futures = []
            # 使用线程池并行处理每个 URL
            with ThreadPoolExecutor(max_workers=16) as executor:
                for url in to_visit:
                    futures.append(executor.submit(htmlParser.getUrls, url))
            
            # 清空待访问列表以准备下一轮
            to_visit = []
            # 等待所有线程处理完成并处理结果
            for future in concurrent.futures.as_completed(futures):
                for c in future.result():
                    # 检查新 URL 是否未被访问且属于同一主机
                    if c not in seen and get_hostname(c) == hostname:
                        seen.add(c)
                        res.append(c)
                        to_visit.append(c)
        return res
</code></pre>
                        
                    </div>
                </section>
            
        </main>
    </div>
    <script src="/static/js/jquery.min.js"></script>
    <script src="/static/js/bootstrap.bundle.min.js"></script>
    <script src="/static/js/highlight.min.js"></script>
    <script>
        document.addEventListener('DOMContentLoaded', function() {
            const copyButtons = document.querySelectorAll('.copy-btn');
            copyButtons.forEach(btn => {
                btn.addEventListener('click', function() {
                    copyCode(this);
                });
            });
            hljs.highlightAll();
        });

        function copyCode(button) {
            const codeBlock = button.previousElementSibling;
            const code = codeBlock.textContent;
            navigator.clipboard.writeText(code).then(function() {
                button.textContent = '已复制';
                setTimeout(function() {
                    button.textContent = '复制代码';
                }, 2000);
            }, function(err) {
                console.error('无法复制代码: ', err);
            });
        }
    </script>
</body>
</html>